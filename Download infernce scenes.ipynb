{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook downloads data from PC in the format required by the inference notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import planetary_computer\n",
    "from pystac.extensions.eo import EOExtension as eo\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shapely\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "from tqdm.auto import tqdm\n",
    "from shapely.geometry import Polygon, shape\n",
    "from shapely.ops import transform\n",
    "from rasterio.merge import merge\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the world tide api key, let me know if you need it\n",
    "world_tide_key_file = Path.cwd() / \"world_tide_key.txt\"\n",
    "world_tides_api_key = world_tide_key_file.read_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is output dir, each scene will end up about 2Gb so make sure you have space!\n",
    "export_dir = Path(\"/Volumes/4TB SSD/Coastline data\") / \"inference_scenes_5\"\n",
    "export_dir.mkdir(exist_ok=True, parents=True)\n",
    "export_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_bands = [\"B03\", \"B08\"]\n",
    "target_bands = 12\n",
    "time_steps = 6\n",
    "extract_start_year = 2022\n",
    "extract_end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the sentinel 2 grid, limited to just Tas at the moment\n",
    "sentinel_2_grid = Path.cwd() / \"data/Senntinele 2 grid coastal tas.gpkg\"\n",
    "s2_grid = gpd.read_file(sentinel_2_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cloud_pct(items_df):\n",
    "    items_df[\"cloud_pct\"] = items_df.apply(\n",
    "        lambda row: row[0].properties[\"eo:cloud_cover\"], axis=1\n",
    "    )\n",
    "    return items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_cloud_pct(items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the world tide api to get the tide height at the time of the image\n",
    "def add_tide_height(centroid, items_df):\n",
    "    results = []\n",
    "    lon, lat = centroid.coords[0]\n",
    "    for id, item in items_df.iterrows():\n",
    "        dt_str = item[0].to_dict()[\"properties\"][\"datetime\"]\n",
    "\n",
    "        dt_obj = datetime.fromisoformat(dt_str.replace(\"Z\", \"+00:00\"))\n",
    "\n",
    "        # Fetch data from API\n",
    "        url = f\"https://www.worldtides.info/api/v3?heights&date={dt_obj.date().isoformat()}&lat={lat}&lon={lon}&key={world_tides_api_key}\"\n",
    "        response = requests.get(url)\n",
    "        data = json.loads(response.text)\n",
    "\n",
    "        min_diff = float(\"inf\")\n",
    "\n",
    "        target_timestamp = dt_obj.timestamp()\n",
    "        for entry in data[\"heights\"]:\n",
    "            diff = abs(entry[\"dt\"] - target_timestamp)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_entry = entry\n",
    "\n",
    "        results.append(closest_entry[\"height\"])\n",
    "    # convert to df and sort by tide height\n",
    "    # results_df = pd.DataFrame(results).sort_values(by=\"tide\", ascending=False)[:limit]\n",
    "    items_df[\"tide_height\"] = results\n",
    "\n",
    "    return items_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign the url and download the band\n",
    "def get_band(href, attempt=0):\n",
    "    try:\n",
    "        singed_href = planetary_computer.sign(href)\n",
    "        with rio.open(singed_href) as src:\n",
    "            return src.read(1), src.profile.copy()\n",
    "    except:\n",
    "        print(f\"Failed to open {href}\")\n",
    "        if attempt < 3:\n",
    "            print(f\"Trying again {attempt+1}\")\n",
    "            return get_band(href, attempt + 1)\n",
    "        else:\n",
    "            print(f\"Failed to open {href} after 3 attempts\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downlaod the required number of bands\n",
    "def downlaod_bands(items_with_tide, time_steps):\n",
    "    bands = []\n",
    "    profile = {}\n",
    "    pbar = tqdm(total=time_steps * len(required_bands), leave=False)\n",
    "    for id, row in items_with_tide.iterrows():\n",
    "        scene_bands = []\n",
    "\n",
    "        for band in required_bands:\n",
    "            href = row[\"item\"].assets[band].href\n",
    "            band, profile = get_band(href)\n",
    "            if type(band) == type(None):\n",
    "                print(f\"Failed to download {href}\")\n",
    "                scene_bands = []\n",
    "                break\n",
    "            pbar.update(1)\n",
    "\n",
    "            scene_bands.append(band)\n",
    "        for band in scene_bands:\n",
    "            bands.append(band)\n",
    "        if len(bands) == time_steps * len(required_bands):\n",
    "            return bands, profile\n",
    "    return bands, profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each grid location may have multiple orbits which covers it, list each orbit\n",
    "def split_by_orbits(items):\n",
    "    orbits = {}\n",
    "    for item in items:\n",
    "        orbit = item.properties[\"sat:relative_orbit\"]\n",
    "        if orbit not in orbits:\n",
    "            orbits[orbit] = [item]\n",
    "        else:\n",
    "            orbits[orbit].append(item)\n",
    "    return orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tif(bands, profile, export_path):\n",
    "    array = np.array(bands)\n",
    "    profile.update(count=array.shape[0])\n",
    "    with rio.open(export_path, \"w\", **profile) as dst:\n",
    "        dst.write(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_grid = s2_grid.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for id, row in s2_grid.iterrows():\n",
    "def download_scene(row):\n",
    "    _, row = row\n",
    "    centroid = row.geometry.centroid\n",
    "    export_path = export_dir / f\"{row.Name}_{extract_start_year}_{extract_end_year}.tif\"\n",
    "    print(export_path)\n",
    "\n",
    "    if export_path.exists():\n",
    "        print(f\"File exists for {row.Name}\")\n",
    "        return\n",
    "\n",
    "    # Sentinel-2 query parameters\n",
    "    query = {\n",
    "        \"collections\": [\"sentinel-2-l2a\"],\n",
    "        \"intersects\": shapely.to_geojson(centroid),\n",
    "        \"datetime\": f\"{extract_start_year}-01-01T00:00:00Z/{extract_end_year}-12-31T23:59:59Z\",\n",
    "        \"query\": {\"s2:mgrs_tile\": {\"eq\": row.Name}},\n",
    "    }\n",
    "    scenes = catalog.search(**query).get_all_items()\n",
    "    # break\n",
    "    if len(scenes) == 0:\n",
    "        return\n",
    "\n",
    "    scenes_by_orbit = split_by_orbits(scenes)\n",
    "    all_orbits_bands = []\n",
    "    for orbit, scenes in scenes_by_orbit.items():\n",
    "        # make df from items in orbit\n",
    "        items_df = pd.DataFrame(scenes)\n",
    "        items_df.columns = [\"item\"]\n",
    "\n",
    "        items_df = add_cloud_pct(items_df)\n",
    "        # sort by cloud cover\n",
    "        items_df = items_df.sort_values(by=\"cloud_pct\", ascending=True)\n",
    "        # only keep the top 20 scenes\n",
    "        items_df = items_df[:20]\n",
    "        items_df = add_tide_height(centroid, items_df)\n",
    "        # round tide height to nearest 10\n",
    "        items_df[\"cloud_pct\"] = items_df[\"cloud_pct\"].apply(\n",
    "            lambda x: round(x / 10) * 10\n",
    "        )\n",
    "        # Sort by cloud_pct and then by tide_height\n",
    "        items_df = items_df.sort_values(\n",
    "            by=[\"cloud_pct\", \"tide_height\"], ascending=[True, False]\n",
    "        )\n",
    "        # download the required bands\n",
    "        bands, profile = downlaod_bands(items_df, time_steps)\n",
    "        all_orbits_bands.append(bands)\n",
    "\n",
    "    all_orbits_bands = np.array(all_orbits_bands)\n",
    "    all_orbits_bands = np.moveaxis(all_orbits_bands, 0, 1)\n",
    "\n",
    "    merged_bands = []\n",
    "    for multi_orbit_bands in all_orbits_bands:\n",
    "        target_array = np.zeros(multi_orbit_bands.shape[1:])\n",
    "        for band in multi_orbit_bands:\n",
    "            target_array[target_array == 0] = band[target_array == 0]\n",
    "        merged_bands.append(target_array)\n",
    "    # merged_bands = np.array(merged_bands)\n",
    "\n",
    "    if len(merged_bands) == target_bands:\n",
    "        export_tif(merged_bands, profile, export_path)\n",
    "    else:\n",
    "        print(f\"Failed to download {row.Name}\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call download_scene with a thread pool\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=4) as pool:\n",
    "    pool.map(download_scene, s2_grid.iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
