{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.inference import run_inference\n",
    "from helpers.download import download_row\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from threading import Thread\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"models/regnety_002_v1.29_model.pkl\")\n",
    "working_dir = Path(\n",
    "    \"/Users/nick/Desktop/CL test\"\n",
    "    # \"/media/nick/SNEAKERNET/Aus working v7\"\n",
    ")\n",
    "model_path.exists(), working_dir.exists()\n",
    "working_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file = Path(\"data/Senntinele 2 grid coastal Aus.gpkg\")\n",
    "grid_gdf = gpd.read_file(grid_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tide_key_path = Path(\"world_tide_key.txt\")\n",
    "tide_key_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_start_year = 2022\n",
    "extract_end_year = 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# failed = []\n",
    "# with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "#     # Submit download tasks\n",
    "#     future_to_id = {\n",
    "#         executor.submit(\n",
    "#             download_row, row, tide_key_path, extract_start_year, extract_end_year\n",
    "#         ): id\n",
    "#         for id, row in grid_gdf.iterrows()\n",
    "#         if not (working_dir / f\"{row['Name']}_2022_2022_pred.tif\").exists()\n",
    "#     }\n",
    "\n",
    "#     for future in tqdm(as_completed(future_to_id), total=len(future_to_id)):\n",
    "#         id = future_to_id[future]\n",
    "#         result = future.result()\n",
    "\n",
    "#         if result is None:\n",
    "#             failed.append(id)\n",
    "#             print(f\"Failed on {id}\")\n",
    "#         else:\n",
    "#             bands, profile = result\n",
    "#             export_path = working_dir / f\"{grid_gdf.loc[id]['Name']}_2022_2022_pred.tif\"\n",
    "#             # Run inference sequentially\n",
    "#             run_inference(model_path, export_path, bands, profile)\n",
    "#             del bands\n",
    "#             del profile\n",
    "#             del result\n",
    "#             gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b886d0e2da834d7db77ad6b8c43e3b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/298 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61fb73c7a3b4c328891ac5bcd20e501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f4dbe408cb4785956f06cb87d82cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-66 (run_inference):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/nick/Documents/Projects/S2Coastline/helpers/inference.py\", line 197, in run_inference\n",
      "    pickle.load(\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/storage.py\", line 337, in _load_from_bytes\n",
      "    return torch.load(io.BytesIO(b))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 1028, in load\n",
      "    return _legacy_load(opened_file, map_location, pickle_module, **pickle_load_args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 1256, in _legacy_load\n",
      "    result = unpickler.load()\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 1193, in persistent_load\n",
      "    wrap_storage=restore_location(obj, location),\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 381, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 274, in _cuda_deserialize\n",
      "    device = validate_cuda_device(location)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/c2m_prod/lib/python3.11/site-packages/torch/serialization.py\", line 258, in validate_cuda_device\n",
      "    raise RuntimeError('Attempting to deserialize object on a CUDA '\n",
      "RuntimeError: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.\n"
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "inf_thread = Thread()\n",
    "for id, row in tqdm(grid_gdf.iterrows(), total=len(grid_gdf)):\n",
    "    # name = row[\"Name\"]\n",
    "    # if name != \"52LDH\":\n",
    "    # continue\n",
    "    export_path = working_dir / f\"{row['Name']}_2022_2022_pred.tif\"\n",
    "    if export_path.exists():\n",
    "        continue\n",
    "    try:\n",
    "        bands, profile = download_row(\n",
    "            row,\n",
    "            tide_key_path,\n",
    "            extract_start_year,\n",
    "            extract_end_year,\n",
    "        )\n",
    "        if inf_thread.is_alive():\n",
    "            inf_thread.join()\n",
    "        if bands is None:\n",
    "            failed.append(id)\n",
    "            print(f\"Failed on {id}\")\n",
    "            continue\n",
    "        inf_thread = Thread(\n",
    "            target=run_inference,\n",
    "            args=(\n",
    "                model_path,\n",
    "                export_path,\n",
    "                bands,\n",
    "                profile,\n",
    "            ),\n",
    "        )\n",
    "        inf_thread.start()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed.append(id)\n",
    "        print(f\"Failed on {id}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gis_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
